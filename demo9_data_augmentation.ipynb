{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d796ad94",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <b>Google Colab Setup:</b> If you're running this notebook on Google Colab, please run the following two cells to mount your Google Drive, set the relevant paths, and change the current working directory. You may skip to the \"Install relevant packages\" cell if you're running the notebook locally.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mount symlink\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/demo9_data_augmentation'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "    %mkdir $DRIVE_PATH\n",
    "\n",
    "## the space in `My Drive` causes some issues,\n",
    "## make a symlink to avoid this\n",
    "SYM_PATH = '/content/demo9_data_augmentation'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "    !ln -s $DRIVE_PATH $SYM_PATH\n",
    "    \n",
    "# Change working directory\n",
    "os.chdir('demo9_data_augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284475a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install relevant packages\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import run_training_loop, test_performance, show_data_augmentations\n",
    "\n",
    "# Set up Jupyter notebook environment\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b41c8e",
   "metadata": {},
   "source": [
    "# CS 182 Demo: Data Augmentations\n",
    "\n",
    "Time and time again, regularization has proven invaluable to machine learning practitioners. With particular focus on this class, we've discussed adapting our models to ensure they do not overfit to the training data. In other words, we want our model to generalize well to unseen data. \n",
    "\n",
    "Many of the approaches discussed --batch normalization, layer normalization, and dropout, to name a few-- involve tuning and adjusting the inner workings of our deep learning architectures. Although Convolutional Neural Networks (CNNs) arose out of a desire for an architecture that had invariances built into it, oftentimes in practice this isn't enough. Data augmentation, the act of modifying our input training data, provides a different approach to regularizing our models.\n",
    "\n",
    "See the following image from Sharon Y. Li's [Stanford AI Lab Blog post](https://ai.stanford.edu/blog/data-augmentation/) for an example of how data augmentation fits into the machine learning pipeline.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ju6SFtwobhE5sEMD9ZeDBcmyVkKwgGkq\" width=\"800px\" align=\"center\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ff796",
   "metadata": {},
   "source": [
    "[Figure 1](https://ai.stanford.edu/blog/data-augmentation/). Data augmentations apply a sequence of transformation functions tuned by human experts to the original data. The augmented data will be used for training downstream models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0d9eb",
   "metadata": {},
   "source": [
    "## Part 1: Data Augmentation in Theory\n",
    "\n",
    "### Acting as a Regularizer\n",
    "Regularization is necessary for deep learning models to generalize well to unseen data (i.e. test data), and it can be introduced into models through explicit methods such as adding a weighted $L_{1}$ or $L_{2}$ penalty, for example, to the loss function. However, regularization can also be introduced implicitly into the model through data augmentation. In previous course material we saw that a least squares problem with data augmentation is equivalent to an $L_{2}$ regularized least squares problem.\n",
    "\n",
    "Given a dataset $D=\\{(x_i, y_i)\\}^{m}_{i=1}$ consisting of $m$ datapoints, where $x_{i} \\in \\mathbb{R}^p$ and $y_{i} \\in \\mathbb{R}$, we can augment our data by adding Gaussian noise such that for each training point $x_{i}$, we have:\n",
    "\n",
    "$$\\tilde{X}_i = x_i + N_i \\text{, where } N_i \\sim \\mathcal{N}(0,\\,\\sigma^{2}I)$$\n",
    "\n",
    "We can construct $\\tilde{X} \\in \\mathbb{R}^{m \\times p}$ by stacking our $\\tilde{X}_i$ terms together and can construct our original design matrix $X \\in \\mathbb{R}^{m \\times p}$ by stacking our $x_{i}$ vectors together. \n",
    "\n",
    "Thus, it can be shown that minimizing the expected least squares objective for the noisy (i.e. augmented) data matrix is equivalent to miniziming the least squares objective with $L_{2}$ regularization: \n",
    "\n",
    "\\begin{equation}\n",
    "   \\arg \\min_{w} \\mathbb{E}[\\|y - \\tilde{X}w\\|^2_{2}] = \\arg \\min_{w}\\frac{1}{m}\\|y - Xw\\|^2_{2} + \\lambda\\|w\\|^2_{2} \\text{, where } \\lambda = \\sigma^{2}\n",
    "\\end{equation}\n",
    "\n",
    "The above result proves the regularizing effect that data augmentation can have on a model. Think about how we can generalize the idea of adding random Gaussian noise from the least squares setting to a computer vision problem. To achieve a similar regularizing effect, one can add random Gaussian noise to each pixel of an image. \n",
    "\n",
    "### Example\n",
    "Consider an edge or a pattern that consistently appears near the center of an image in a subset of the training data. For example, this might be the stripes of a zebra consistently appearing in the center, due to the images of the zebra consistently being centered on the zebra. The model (i.e. CNN) will latch onto that edge or pattern as it is designed to do. Due to translational/equivariance invariance, the model should be able to detect if the zebra and its stripes were shifted around the image, thus providing the model with information to inform its prediction (ideally, that would be 'zebra').\n",
    "\n",
    "However, in reality, images are not always as clean-cut as the training examples in datasets such as CIFAR-10. The quality of the image could be poor (blurriness) resulting in the model poorly identifying features, or a glare in the picture that distracts the model from important patterns and edges that result in a correct classification. It might even be possible that the subset of zebra images are entirely centered on a docile zebra standing horizontally, meaning a rotated or 'active' zebra may result in a misclassification.\n",
    "\n",
    "Thus, data augmentation during training provides a way of implicitly regularizing the model by artificially creating scenarios that could be realistically seen in the real world. These data augmentations force the model to adapt to these changes by relying less on exploiting patterns from idealized versions of images. By augmenting our data, we are approximating what our data looks like in the real world.\n",
    "\n",
    "Data augmentation is also an avenue for domain knowledge to be exploited to help produce more accurate while still robust models. Experts and scientists can provide encodings of what is important via domain emphasis by selecting data augmentations that accurately represent what the model may encounter post-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a8736",
   "metadata": {},
   "source": [
    "## Part 2: Basic Augmentations\n",
    "\n",
    "These augmentations are used to promote invariance to small semantically insignificant changes. A few basic augmentations are:\n",
    "1. Random Cropping\n",
    "2. Rescaling\n",
    "3. Rotations\n",
    "3. Subset\n",
    "4. Color Adjustment\n",
    "5. Blurring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18365e4",
   "metadata": {},
   "source": [
    "### Using PyTorch for Data Augmentation\n",
    "\n",
    "There are many data augmentations that are implemented in the `torchvision.transforms` modules. Below are a few examples of common data augmentations used in CNNs. Let's load in a public domain image of a [Golden Retriever](https://www.publicdomainpictures.net/en/view-image.php?image=35696&picture=golden-retriever-dog) to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e61d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image as NumPy array\n",
    "dog = plt.imread(fname=\"images/dog.jpg\", format=None)\n",
    "\n",
    "# Convert NumPy array to Tensor\n",
    "dog = torch.from_numpy(dog)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(dog)\n",
    "ax.set_xticks(ticks=[])\n",
    "ax.set_yticks(ticks=[])\n",
    "plt.title(\"Original Image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b1a8d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <b>Permuting Your Images: </b>The transformations found within the <code>transforms</code> library expect RGB images to be of shape $(3, H, W)$ but the <code>imshow</code> method from <code>matplotlib.pyplot</code> expects RGB images of shape $(H, W, 3)$. Thus, we must use PyTorch's <code>permute</code> method to reorder the images' dimensions into the correct shape.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561a1df",
   "metadata": {},
   "source": [
    "### Random Rotation\n",
    "Use the `transforms.RandomRotation` method to randomly rotate an image up to a certain degree. You can find the official PyTorch documentation [here](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html).\n",
    "\n",
    "**Example:** Random rotations work to introduce rotational invariance and equivariance to your model. Think about the case of recognizing handwritten digits. A slightly tilted \"7\" should still be classified as a \"7\". By randomly rotating our original data, we are forcing the model to be invariant to the rotation angle of the \"7\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum degree you'd like to rotate the image by\n",
    "MAX_DEGREE = 45\n",
    "\n",
    "# Rotate image using rotation transformation\n",
    "rotation = transforms.RandomRotation(degrees=MAX_DEGREE,\n",
    "                                     interpolation=transforms.functional.InterpolationMode.NEAREST,\n",
    "                                     expand=False,\n",
    "                                     center=None,\n",
    "                                     fill=0)\n",
    "\n",
    "# Randomly rotate the original image four times\n",
    "show_data_augmentations(original_im=dog, transform_f=rotation, title=\"Rotated Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552fca8",
   "metadata": {},
   "source": [
    "### Random Crop\n",
    "Use the `transforms.RandomResizedCrop` method to crop the given image at a random location and then resize to match the original image size. Oftentimes, we fix our model architecture such that it always takes in the same size input image, thus it's important to resize after cropping. You can find the official PyTorch documentation [here](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html).\n",
    "\n",
    "**Example:** Random crops help to introduce scale invariance to our models since the objects in our training data may not always be of the same scale. Think about our toy example of classifying handwritten digits. Sometimes we might get a \"7\" written quite small, but other times the \"7\" may take up the entire image. Other times, we may not see the entirety of the \"7\"; adding random crops helps the model to generalize to different scales of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a fresh, untransformed image\n",
    "dog = torch.from_numpy(plt.imread(fname=\"images/dog.jpg\", format=None))\n",
    "\n",
    "# Crop the input image based on crop size\n",
    "crop = transforms.RandomResizedCrop(size=dog.shape[:2],\n",
    "                                    scale=(0.08, 1.0),\n",
    "                                    ratio=(0.75, 1.3333333333333333),\n",
    "                                    interpolation=transforms.functional.InterpolationMode.BILINEAR,\n",
    "                                    antialias='warn')\n",
    "\n",
    "# Randomly crop the original image four times\n",
    "show_data_augmentations(original_im=dog, transform_f=crop, title=\"Cropped Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63f085",
   "metadata": {},
   "source": [
    "Notice that the image size before and after cropping is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb87bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before Crop: {dog.shape}')\n",
    "print(f'After Crop: {crop(dog.permute(2, 0, 1)).permute(1, 2, 0).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642cb0e",
   "metadata": {},
   "source": [
    "### Gaussian Blur\n",
    "Use the `transforms.GaussianBlur` method to blur the image with randomly chosen Gaussian blur. You can find the official PyTorch documentation [here](https://pytorch.org/vision/main/generated/torchvision.transforms.GaussianBlur.html).\n",
    "\n",
    "**Example:** The application of smoothing operators such as Gaussian filters to input images is often used to eliminate the noisy, high-frequency components. Say you have a grainy image of a dog and want to use a CNN to classify the image correctly. By convolving the image with a Gaussian kernel, we are filtering out the grainy, high-frequency components, thus leaving only a blurred version of the dog; doing so helps the model learn only the most important features of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d697ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a fresh, untransformed image\n",
    "dog = torch.from_numpy(plt.imread(fname=\"images/dog.jpg\", format=None))\n",
    "\n",
    "# Set kernel size and variance\n",
    "KERNEL_SIZE=13\n",
    "SIGMA_RANGE=(0.1,200)\n",
    "\n",
    "# Pass a Gaussian filter over the image\n",
    "blur = transforms.GaussianBlur(kernel_size=KERNEL_SIZE, sigma=SIGMA_RANGE)\n",
    "\n",
    "# Randomly blur the original image four times\n",
    "show_data_augmentations(original_im=dog, transform_f=blur, title=\"Blurred Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d0120",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Try it out:</b> Play around with the <code>KERNEL_SIZE</code> and <code>SIGMA_RANGE</code> variables. How do both the kernel size and the sigma value affect the resulting blur? What happens if you use a small kernel with a large sigma and vice versa?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c5b46",
   "metadata": {},
   "source": [
    "### Color Jitter\n",
    "\n",
    "Use the `transforms.ColorJitter` method to randomly adjusted brightness, contrast, saturation and hue of an image. You can find the official PyTorch documentation [here](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html).\n",
    "\n",
    "**Example:** Adding color jitter to your training images allows the network to generalize to many different real-world scenarios by allowing the model to be invariant to the lighting conditions. When trying to classify an input image as a \"dog\", we don't want the network to rely on images of dogs that were taken in ideal lighting conditions. Both a bright and dimly-lit image of a dog should be classified as \"dog\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45334b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a fresh, untransformed image\n",
    "dog = torch.from_numpy(plt.imread(fname=\"images/dog.jpg\", format=None))\n",
    "\n",
    "# Set ColorJitter parameters\n",
    "BRIGHTNESS = 0.5\n",
    "CONTRAST = 0.5\n",
    "SATURATION = 0.1\n",
    "HUE = 0.05\n",
    "\n",
    "# Add color jitter to the image\n",
    "jitter = transforms.ColorJitter(brightness=BRIGHTNESS, contrast=CONTRAST, saturation=SATURATION, hue=HUE)\n",
    "\n",
    "# Randomly jitter the original image four times\n",
    "show_data_augmentations(original_im=dog, transform_f=jitter, title=\"Jittered Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746a032",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Augmentations\n",
    "\n",
    "Advanced data augmentations provide more complex and unique training examples to further regularize our model. The real world is complicated and variable, meaning unseen (by the model) data are likely to be complex and dynamic. Thus, advanced augmentations provide practitioners an ability to introduce more heavily augmented, and sometimes more realistic, data to further improve the model's robustness.\n",
    "\n",
    "### Composing Multiple Augmentations \n",
    "\n",
    "As machine learning practitioners, we've found that composing multiple data augmentations together, most often within a PyTorch `DataSet`, helps to improve performance and regularizes the model. Below is an example of one such composition of basic data augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a fresh, untransformed image\n",
    "dog = torch.from_numpy(plt.imread(fname=\"images/dog.jpg\", format=None))\n",
    "\n",
    "composed_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=20, \n",
    "                                  interpolation=transforms.functional.InterpolationMode.NEAREST, \n",
    "                                  expand=False, \n",
    "                                  center=None, \n",
    "                                  fill=0),\n",
    "        transforms.GaussianBlur(kernel_size=13, sigma=7),\n",
    "        transforms.ColorJitter(brightness=0.3, \n",
    "                               contrast=0.3, \n",
    "                               saturation=0.1,\n",
    "                               hue=0.1)\n",
    "        ])\n",
    "\n",
    "# Randomly augment the original image four times\n",
    "show_data_augmentations(original_im=dog, transform_f=composed_transform, title=\"Augmented Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004bb33b",
   "metadata": {},
   "source": [
    "### More Aggressive Data Augmentations\n",
    "\n",
    "As shown in lecture, there are several more aggressive data augmentations that are used in practice. Empirically, they are found to have a regularizing effect on the model. Some examples are below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1x_ednmeFKO-iWficKQdcMnlRaIsHiJ3r\" width=\"800px\" align=\"center\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d825ea",
   "metadata": {},
   "source": [
    "You can read more about the above data augmentations in the original [PixMix paper](https://arxiv.org/pdf/2112.05135.pdf) from 2022, but we will not explicitly cover them in practice within this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea9f08",
   "metadata": {},
   "source": [
    "## Part 4: Augmentations in Practice\n",
    "\n",
    "To showcase the empirical effects of data augmentation in practice, run through the following example which utilizes a ResNet achitecture for an image classification task using the CIFAR-10 dataset.\n",
    "\n",
    "### CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset is a collection of 60,000 labeled 32x32 color images that is commonly used for training computer vision models. Each image is labeled with a class from one of the following categories: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. CIFAR-10 is commonly used as a baseline for a modelâ€™s image classification abilities, and we will be using it to train and compare a base model and a data augmented model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30324fbc",
   "metadata": {},
   "source": [
    "### Loading the Model Architecture\n",
    "\n",
    "To show the empirical effects of data augmentations in practice, we'll utilize the ResNet-18 architecture with untrained weights. The original [ResNet](https://arxiv.org/abs/1512.03385) architecture allowed for much deeper models due to its innovative approach towards solving the \"vanishing gradient\" problem: the skip connection. ResNet has proven to be one of the most successful architectures used for object classification and recognition. You can examine the specific layers and parameters below.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1U3X5I40imN2RRsidPh1_BT1pd-DqWWiz\" width=\"600px\" align=\"center\"></img>\n",
    "\n",
    "[Figure 2.](https://www.pluralsight.com/guides/introduction-to-resnet) The ResNet-18 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fd719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to load the model\n",
    "model = torchvision.models.resnet18(weights=None)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c6756",
   "metadata": {},
   "source": [
    "We will use the same ResNet-18 architecture to train two models: a base model with no data augmentation, and another model trained with data augmentation. Below, we use methods from the `torchvision.transforms` module to compose a set of transformations that we can apply to the training data. We augment the data by performing one random horizontal flip followed by one random crop. The PyTorch code is shown below:\n",
    "\n",
    "**Transformation Code**\n",
    "```python\n",
    "data_aug_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.25),\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f84c4d",
   "metadata": {},
   "source": [
    "### Training the Models\n",
    "\n",
    "For the sake of time and computation, we have ommitted the training process, and instead have loaded in the trained models below. To reproduce the process and model training, please reference the `model_train.py` and `utils.py` files located at the root of the directory.\n",
    "\n",
    "For reference, our training hyperparameters are described below:\n",
    "```python\n",
    "batch_size=128\n",
    "num_epochs=15\n",
    "learning_rate=0.001\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "More information about the ADAM optimizer can be found on the official PyTorch documentation [here](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models from models folder\n",
    "base_model = torch.load(\"models/resnet18_base.pt\", map_location=\"cpu\")\n",
    "aug_model = torch.load(\"models/resnet18_data_augment.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55832341",
   "metadata": {},
   "source": [
    "Let's take a look at the training and validation loss curves for each model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d3bb2",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1oHsuSsSX4mBXFSHqmpYV7LWu-wIfTIco\" width=\"800px\" align=\"left\"></img>\n",
    "<img src=\"https://drive.google.com/uc?id=1el9SeHIfo0KVMGEynMkIih9qnAbk_yvO\" width=\"800px\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cdadf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>What do you notice?</b> Consider the trends of the train and validation curves in respect to each other.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1fda3",
   "metadata": {},
   "source": [
    "Now, let's examine how each model performs on the test dataset. First, we must load in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_test = torchvision.datasets.CIFAR10(root = \"data\", \n",
    "                                            train=False, \n",
    "                                            download = True, \n",
    "                                            transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55de644",
   "metadata": {},
   "source": [
    "Run the following cells to test the performances of both the base model and data augmented model. You'll see that the model trained using the data augmentations performs better than the base model on unseen test data. You can find the `test_performance` function in the `utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take up to a minute to run\n",
    "test_performance(model=base_model, test_data=cifar10_test, batch_size=128, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd120d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take up to a minute to run\n",
    "test_performance(model=aug_model, test_data=cifar10_test, batch_size=128, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c7ea2",
   "metadata": {},
   "source": [
    "Although the base model's training loss was nearly half that of the data augmented model's training loss, the base model's performance on the validation set and test set was poor. On the other hand, the data augmented model's performance and accuracy on the test set was better than the base model. This example displays the regularization effect data augmentation has on model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
